version: '3'

services:

  # APPLICATIONS LOAD BALANCER
  loadbalancer:
    image: nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    command: [ nginx-debug, '-g', 'daemon off;' ]
    ports:
      - "8080:80"
    restart: always
    networks:
      - mynetwork
    depends_on:
      - activemq
      - scheduler-instance1
      - consumer-instance1
      - accounts-instance1
      - accounts-instance2
      - accounts-instance3
      - accounts-instance4
      - processor-instance1

  # JMS BROKER
  activemq:
    image: quay.io/artemiscloud/activemq-artemis-broker:dev.latest
    ports:
      - 61616:61616
      - 8161:8161
    restart: always
    environment:
      - AMQ_USER=admin
      - AMQ_PASSWORD=admin
    networks:
      - mynetwork
  kafka:
    image: confluentinc/cp-kafka:6.0.14
    depends_on:
      - zookeeper
    ports:
      - '29092:29092'
      - '9092:9092'
    restart: always
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka:9092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_MESSAGE_MAX_BYTES: 52428800 # Define o tamanho máximo de uma mensagem para 50MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 52428800 # Configura o limite para replicação

      KAFKA_PRODUCER_MAX_REQUEST_SIZE: 52428800
      KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
      CONNECT_PRODUCER_MAX_REQUEST_SIZE: 52428800
      CONNECT_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
    networks:
      - mynetwork
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8085:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - mynetwork
  zookeeper:
    image: confluentinc/cp-zookeeper:6.0.14
    ports:
      - '2181:2181'
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - mynetwork

  # APPLICATIONS
  scheduler-instance1:
    image: fernandoaraujo/scheduler:latest
    build:
      context: .
      dockerfile: scheduler/Dockerfile
      args:
        - HOME=${HOME}
#    volumes:
#      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8081:8081"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - REQUEST_URL_CONSUMER=http://172.17.0.1:8080/accounts/transactions
      - OTEL_SERVICE_NAME=scheduler
  consumer-instance1:
    image: fernandoaraujo/consumer:latest
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    build:
      context: .
      dockerfile: consumer/Dockerfile
      args:
        - HOME=${HOME}
#    volumes:
#      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8082:8082"
    ports:
      - "5002:5002"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - REQUEST_URL_ACCOUNTS=http://172.17.0.1:8080/accounts/transactions
      - OTEL_SERVICE_NAME=consumer
  accounts-instance1:
    image: fernandoaraujo/accounts:latest
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    build:
      context: .
      dockerfile: accounts/Dockerfile
      args:
        - HOME=${HOME}
#    volumes:
#      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8083:8083"
    ports:
      - "5003:5002"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - OTEL_SERVICE_NAME=accounts-instance1
  accounts-instance2:
    image: fernandoaraujo/accounts:latest
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    build:
      context: .
      dockerfile: accounts/Dockerfile
      args:
        - HOME=${HOME}
#    volumes:
#      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8083:8083"
    ports:
      - "5004:5002"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - OTEL_SERVICE_NAME=accounts-instance2
  accounts-instance3:
    image: fernandoaraujo/accounts:latest
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    build:
      context: .
      dockerfile: accounts/Dockerfile
      args:
        - HOME=${HOME}
    #    volumes:
    #      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8083:8083"
    ports:
      - "5005:5002"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - OTEL_SERVICE_NAME=accounts-instance3
  accounts-instance4:
    image: fernandoaraujo/accounts:latest
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    build:
      context: .
      dockerfile: accounts/Dockerfile
      args:
        - HOME=${HOME}
    #    volumes:
    #      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8083:8083"
    ports:
      - "5006:5002"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - OTEL_SERVICE_NAME=accounts-instance4
  processor-instance1:
    image: fernandoaraujo/processor:latest
    build:
      context: .
      dockerfile: processor/Dockerfile
      args:
        - HOME=${HOME}
#    volumes:
#      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
    expose:
      - "8084:8084"
    ports:
      - "5007:5002"
    restart: always
    env_file:
      - docker/global-applications.env
    environment:
      - OTEL_SERVICE_NAME=processor

  # OBSERVABILITY
#  jaeger:
#    networks:
#      - mynetwork
#    image: jaegertracing/all-in-one:1.46
#    volumes:
#      - /tmp/badger:/badger
#      - ./docker/observability/jaeger-ui.json:/etc/jaeger/jaeger-ui.json
#    command: --query.ui-config /etc/jaeger/jaeger-ui.json
#    environment:
#      - LOG_LEVEL=info
#      - METRICS_STORAGE_TYPE=prometheus
#      - PROMETHEUS_SERVER_URL=http://prometheus:9090
#      - PROMETHEUS_QUERY_SUPPORT_SPANMETRICS_CONNECTOR=true
#      - PROMETHEUS_QUERY_NAMESPACE=spanmetrics
#      - PROMETHEUS_QUERY_DURATION_UNIT=s
#      - SPAN_STORAGE_TYPE=badger
#      - BADGER_EPHEMERAL=false
#      - BADGER_DIRECTORY_VALUE=/badger/data
#      - BADGER_DIRECTORY_KEY=/badger/key
#    ports:
#      - "16686:16686" # serve frontend
#      - "14250:14250" # accept model.proto
#  otel_collector:
#    networks:
#      - mynetwork
#    image: otel/opentelemetry-collector-contrib:0.72.0
#    volumes:
#      - ./docker/observability/otel-collector-config.yml:/etc/otelcol/otel-collector-config.yml
#    command: --config /etc/otelcol/otel-collector-config.yml
#    ports:
#      - "8888:8888"   # Prometheus metrics exposed by the collector
#      - "8889:8889"   # Prometheus exporter metrics
#      - "4317:4317"   # OTLP gRPC receiver
#      - "4318:4318"   # OTLP http receiver
#    restart: always
  prometheus:
    networks:
      - mynetwork
    image: prom/prometheus:latest
    volumes:
      - /tmp/prometheus:/prometheus
      - ./docker/observability/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"
  grafana:
    networks:
      - mynetwork
    image: grafana/grafana:10.0.1
    volumes:
      - ./docker/observability/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./docker/observability/grafana/grafana-datasource.yml:/etc/grafana/provisioning/datasources/datasource.yaml
      - ./docker/observability/grafana/dashboard.yml:/etc/grafana/provisioning/dashboards/main.yaml
      - ./docker/observability/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    ports:
      - "3000:3000"
  node_exporter:
    image: quay.io/prometheus/node-exporter:v1.8.0
    container_name: node_exporter
    command:
      - '--path.rootfs=/host'
    network_mode: host
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: cadvisor
    ports:
      - 8095:8080
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro

  # ELK Stack
#  setup:
#    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
#    user: "0"
#    command: >
#      bash -c '
#        echo "Waiting for Elasticsearch availability";
#        until curl -s http://elasticsearch:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
#        echo "Setting kibana_system password";
#        until curl -s -X POST  -u "elastic:change_me" -H "Content-Type: application/json" http://elasticsearch:9200/_security/user/kibana_system/_password -d "{\"password\":\"change_me\"}" | grep -q "^{}"; do sleep 10; done;
#        echo "All done!";
#      '
#    healthcheck:
#      test: ["CMD-SHELL", "[ -f config/certs/elasticsearch/elasticsearch.crt ]"]
#      interval: 1s
#      timeout: 5s
#      retries: 120
#  # Final storage: Elasticsearch
#  elasticsearch:
##    depends_on:
##      setup:
##        condition: service_healthy
#    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
#    container_name: elasticsearch
#    environment:
#      - node.name=elasticsearch
#      - path.logs=/var/log/
#      - cluster.name=elasticsearch
#      - discovery.type=single-node
#      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
#      - ELASTICSEARCH_USERNAME=kibana_system
#      - ELASTIC_PASSWORD=change_me
#      - bootstrap.memory_lock=true
#      - xpack.security.enabled=true
#      - xpack.security.authc.api_key.enabled=true
#    ports:
#      - 9200:9200
##    healthcheck:
##      test:
##        [
##          "CMD-SHELL",
##          "curl -s -I http://localhost:9200/_cluster/health || exit 1"
##        ]
##      interval: 10s
##      timeout: 10s
##      retries: 120
#
#  # Kibana to display monitoring data
#  kibana:
##    depends_on:
##      elasticsearch:
##        condition: service_healthy
#    image: docker.elastic.co/kibana/kibana:8.12.2
#    container_name: kibana
#    environment:
#      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
#      - ELASTIC_APM_ACTIVE=true
#      - ELASTIC_APM_SERVER_URL=http://apm-server:8200
#      - ELASTICSEARCH_USERNAME=kibana_system
#      - ELASTICSEARCH_PASSWORD=change_me
#    ports:
#      - 5601:5601
#    volumes:
#      - ./docker/observability/kibana.yml:/usr/share/kibana/config/kibana.yml
##    healthcheck:
##      test:
##        [
##          "CMD-SHELL",
##          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
##        ]
##      interval: 10s
##      timeout: 10s
##      retries: 120
#
#  # Kibana to display monitoring data
#  apm-server:
#    image: docker.elastic.co/apm/apm-server:8.12.2
#    container_name: apm-server
#    ports:
#      - 8200:8200
#    command: >
#      apm-server -e
#        -E apm-server.rum.enabled=true
#        -E setup.kibana.host=kibana:5601
#        -E setup.template.settings.index.number_of_replicas=0
#        -E apm-server.kibana.enabled=true
#        -E apm-server.kibana.host=kibana:5601
#        -E apm-server.kibana.protocol=http
#        -E apm-server.kibana.username=elastic
#        -E apm-server.kibana.password=change_me
#        -E output.elasticsearch.hosts=["http://elasticsearch:9200"]
#        -E output.elasticsearch.username=elastic
#        -E output.elasticsearch.password=change_me
##    healthcheck:
##      interval: 10s
##      retries: 120
##      test: curl -I --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null https://localhost:8200/
##    depends_on:
##      elasticsearch:
##        condition: service_healthy

networks:
  mynetwork:

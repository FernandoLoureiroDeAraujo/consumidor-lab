version: '3'

services:

  # APPLICATIONS LOAD BALANCER
  loadbalancer:
    image: nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    command: [ nginx-debug, '-g', 'daemon off;' ]
    ports:
      - "8080:80"
    networks:
      - mynetwork
    depends_on:
      - activemq
      - scheduler-instance1
#      - consumer-instance1
#      - accounts-instance1
#      - accounts-instance2
#      - processor-instance1

  # JMS BROKER
  activemq:
    image: quay.io/artemiscloud/activemq-artemis-broker:dev.latest
    ports:
      - 61616:61616
      - 8161:8161
    environment:
      - AMQ_USER=admin
      - AMQ_PASSWORD=admin
    networks:
      - mynetwork

  # KAFKA
#  zookeeper:
#    image: confluentinc/cp-zookeeper:7.3.2
#    hostname: zookeeper
#    container_name: zookeeper
#    ports:
#      - 2181:2181
#    environment:
#      ZOOKEEPER_CLIENT_PORT: 2181
#      ZOOKEEPER_SERVER_ID: 1
#      ZOOKEEPER_SERVERS: zookeeper:2888:3888
#  kafka:
#    image: confluentinc/cp-kafka:7.3.2
#    hostname: kafka
#    container_name: kafka
#    ports:
#      - 9092:9092
#      - 29092:29092
#      - 9999:9999
#    environment:
#      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
#      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
#      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
#      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
#      KAFKA_BROKER_ID: 1
#      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
#      KAFKA_JMX_PORT: 9999
#      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
#      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
#      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
#    depends_on:
#      - zookeeper
  kafka:
    image: confluentinc/cp-kafka:6.0.14
    depends_on:
      - zookeeper
    ports:
      - '29092:29092'
      - '9092:9092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka:9092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - mynetwork
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8085:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - mynetwork
  zookeeper:
    image: confluentinc/cp-zookeeper:6.0.14
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - mynetwork

  # APPLICATIONS
  scheduler-instance1:
    build:
      context: .
      dockerfile: scheduler/Dockerfile
      args:
        - HOME=${HOME}
#    volumes:
#      - "${HOME}/.m2:/root/.m2"
    networks:
      - mynetwork
    depends_on:
      - activemq
#      - jaeger
    expose:
      - "8081:8081"
    ports:
      - "5001:5002"
    env_file:
      - docker/global-applications.env
    environment:
      - REQUEST_URL_CONSUMER=http://172.17.0.1:8080/accounts/transactions
      - OTEL_SERVICE_NAME=scheduler
#  consumer-instance1:
#    build:
#      context: .
#      dockerfile: consumer/Dockerfile
#      args:
#        - HOME=${HOME}
##    volumes:
##      - "${HOME}/.m2:/root/.m2"
#    networks:
#      - mynetwork
#    depends_on:
#      - activemq
#      - jaeger
#    expose:
#      - "8082:8082"
#    ports:
#      - "5002:5002"
#    env_file:
#      - docker/global-applications.env
#    environment:
#      - REQUEST_URL_ACCOUNTS=http://172.17.0.1:8080/accounts/transactions
#      - OTEL_SERVICE_NAME=consumer
#  accounts-instance1:
#    build:
#      context: .
#      dockerfile: accounts/Dockerfile
#      args:
#        - HOME=${HOME}
##    volumes:
##      - "${HOME}/.m2:/root/.m2"
#    networks:
#      - mynetwork
#    depends_on:
#      - activemq
##      - jaeger
#    expose:
#      - "8083:8083"
#    ports:
#      - "5003:5002"
#    env_file:
#      - docker/global-applications.env
#    environment:
#      - OTEL_SERVICE_NAME=accounts-instance1
#  accounts-instance2:
#    build:
#      context: .
#      dockerfile: accounts/Dockerfile
#      args:
#        - HOME=${HOME}
##    volumes:
##      - "${HOME}/.m2:/root/.m2"
#    networks:
#      - mynetwork
#    depends_on:
#      - activemq
#      - jaeger
#    expose:
#      - "8083:8083"
#    ports:
#      - "5004:5002"
#    env_file:
#      - docker/global-applications.env
#    environment:
#      - OTEL_SERVICE_NAME=accounts-instance2
#  processor-instance1:
#    build:
#      context: .
#      dockerfile: processor/Dockerfile
#      args:
#        - HOME=${HOME}
##    volumes:
##      - "${HOME}/.m2:/root/.m2"
#    networks:
#      - mynetwork
#    depends_on:
#      - activemq
#      - jaeger
#    expose:
#      - "8084:8084"
#    ports:
#      - "5005:5002"
#    env_file:
#      - docker/global-applications.env
#    environment:
#      - OTEL_SERVICE_NAME=processor

  # OBSERVABILITY
  jaeger:
    networks:
      - mynetwork
    image: jaegertracing/all-in-one:latest
    volumes:
      - ./docker/observability/jaeger-ui.json:/etc/jaeger/jaeger-ui.json
    command: --query.ui-config /etc/jaeger/jaeger-ui.json
    environment:
      - LOG_LEVEL=info
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
      - PROMETHEUS_QUERY_SUPPORT_SPANMETRICS_CONNECTOR=true
      - PROMETHEUS_QUERY_NAMESPACE=spanmetrics
      - PROMETHEUS_QUERY_DURATION_UNIT=s
    ports:
      - "16686:16686" # serve frontend
      - "14250:14250" # accept model.proto
  otel_collector:
    networks:
      - mynetwork
    image: otel/opentelemetry-collector-contrib:latest
    volumes:
      - ./docker/observability/otel-collector-config.yml:/etc/otelcol/otel-collector-config.yml
    command: --config /etc/otelcol/otel-collector-config.yml
    ports:
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP http receiver
    depends_on:
      - jaeger
  prometheus:
    networks:
      - mynetwork
    image: prom/prometheus:latest
    volumes:
      - ./docker/observability/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  grafana:
    networks:
      - mynetwork
    image: grafana/grafana:latest
    volumes:
      - ./docker/observability/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./docker/observability/grafana/grafana-datasource.yml:/etc/grafana/provisioning/datasources/datasource.yaml
      - ./docker/observability/grafana/dashboard.yml:/etc/grafana/provisioning/dashboards/main.yaml
      - ./docker/observability/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    ports:
      - "3000:3000"

  # ELK Stack
  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
    user: "0"
    command: >
      bash -c '
        echo "Waiting for Elasticsearch availability";
        until curl -s http://elasticsearch:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST  -u "elastic:change_me" -H "Content-Type: application/json" http://elasticsearch:9200/_security/user/kibana_system/_password -d "{\"password\":\"change_me\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/elasticsearch/elasticsearch.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120
  # Final storage: Elasticsearch
  elasticsearch:
#    depends_on:
#      setup:
#        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
    container_name: elasticsearch
    environment:
      - node.name=elasticsearch
      - path.logs=/var/log/
      - cluster.name=elasticsearch
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTIC_PASSWORD=change_me
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.authc.api_key.enabled=true
    ports:
      - 9200:9200
#    healthcheck:
#      test:
#        [
#          "CMD-SHELL",
#          "curl -s -I http://localhost:9200/_cluster/health || exit 1"
#        ]
#      interval: 10s
#      timeout: 10s
#      retries: 120

  # Kibana to display monitoring data
  kibana:
#    depends_on:
#      elasticsearch:
#        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:8.12.2
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTIC_APM_ACTIVE=true
      - ELASTIC_APM_SERVER_URL=http://apm-server:8200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=change_me
    ports:
      - 5601:5601
    volumes:
      - ./docker/observability/kibana.yml:/usr/share/kibana/config/kibana.yml
#    healthcheck:
#      test:
#        [
#          "CMD-SHELL",
#          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
#        ]
#      interval: 10s
#      timeout: 10s
#      retries: 120

  # Kibana to display monitoring data
  apm-server:
    image: docker.elastic.co/apm/apm-server:8.12.2
    container_name: apm-server
    ports:
      - 8200:8200
    command: >
      apm-server -e
        -E apm-server.rum.enabled=true
        -E setup.kibana.host=kibana:5601
        -E setup.template.settings.index.number_of_replicas=0
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=kibana:5601
        -E apm-server.kibana.protocol=http
        -E apm-server.kibana.username=elastic
        -E apm-server.kibana.password=change_me
        -E output.elasticsearch.hosts=["http://elasticsearch:9200"]
        -E output.elasticsearch.username=elastic
        -E output.elasticsearch.password=change_me
#    healthcheck:
#      interval: 10s
#      retries: 120
#      test: curl -I --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null https://localhost:8200/
#    depends_on:
#      elasticsearch:
#        condition: service_healthy

networks:
  mynetwork:
